import json
import shutil
import pandas as pd
import vertexai
import glob
import os
import requests
from Email_extraction import extractor
from pdf2image import convert_from_path
from vertexai.preview.language_models import TextGenerationModel

# Extract emails initially
extractor()

# Docling API URL
DOCLING_API_URL = "http://127.0.0.1:5000/convert"

def PDF2Image(pdf_file):
    """
    Converts PDF files to images.
    """
    poppler_path = r'C:\poppler-23.11.0\Library\bin'

    images_list = []
    images = convert_from_path(pdf_file, poppler_path=poppler_path)
    path = os.path.join(os.getcwd(), "test")
    if not os.path.exists(path):
        os.mkdir(path)

    for j, image in enumerate(images):
        image_path = os.path.join("test", f"{os.path.basename(pdf_file).split('.')[0]}_{j}.png")
        image.save(image_path)
        images_list.append(image_path)

    return images_list

def docling_ocr(file_path):
    """
    Sends the file to Docling API for text extraction.
    """
    try:
        with open(file_path, 'rb') as file:
            response = requests.post(DOCLING_API_URL, files={'file': file})

        if response.status_code == 200:
            extracted_text = response.json().get('text', '')
            print(extracted_text)
            return extracted_text
        else:
            print(f"Docling API Error: {response.status_code} - {response.text}")
            return ""
    except Exception as e:
        print(f"Docling OCR Error: {e}")
        return ""

def Extraction(text):
    """
    Uses Vertex AI for extracting structured data from text.
    """
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "email-extraction-381718-3f73208ce3b71.json"
    vertexai.init(project="email-extraction-381718", location="us-central1")

    model_prediction = TextGenerationModel.from_pretrained("text-bison-32k")
    parameters = {
        "temperature": 0.2,
        "max_output_tokens": 2048,
        "top_p": 0.4,
        "top_k": 30
    }
    prompt = """
        You are a fully trained extraction bot specialized in resumes.
        Your task is to identify candidate details in the document.
        Extract the following keys: 
        Name, Qualification, College Name, Passing year, Percentage, Phone, Email, Technologies.
        If data is missing, use "NULL".
        """
    response = model_prediction.predict(prompt + text, **parameters)
    return response.text

# Process DOC and DOCX files
all_data = []
doc_file_list = glob.glob(os.path.join(os.getcwd(), "Base_Folder", "*.doc"))
docx_file_list = glob.glob(os.path.join(os.getcwd(), "Base_Folder", "*.docx"))
docx_file_list.extend(doc_file_list)

for docx_file in docx_file_list:
    if docx_file.endswith('doc'):
        from doc2docx import convert
        docx_file_name = os.path.basename(docx_file).split(".")[0] + ".docx"
        docx_file_path = os.path.join(os.getcwd(), "Base_Folder", docx_file_name)
        convert(docx_file, docx_file_path)
        os.remove(docx_file)
    else:
        docx_file_path = docx_file

    output_file_name = os.path.basename(docx_file_path).split(".")[0] + ".pdf"
    output_file_path = os.path.join(os.getcwd(), "Base_Folder", output_file_name)
    from docx2pdf import convert
    convert(docx_file_path, output_file_path)
    os.remove(docx_file_path)

# Process PDF files
pdf_file_list = glob.glob(os.path.join(os.getcwd(), "Base_Folder", "*.pdf"))
for single_pdf in pdf_file_list:
    try:
        single_pdf_img = PDF2Image(single_pdf)
        text = ""
        for single_img in single_pdf_img:
            text += docling_ocr(single_img)  # Using Docling OCR instead of PaddleOCR

        data = Extraction(text)
        json_data = json.loads(data)
        print(json_data)
        df = pd.DataFrame.from_dict(json_data, orient='index').T
        all_data.append(df)

        # Clean up images
        for img in single_pdf_img:
            os.remove(img)

        # Move processed PDF to the "Processed" folder
        final_dir = os.path.join(os.getcwd(), "Processed")
        if not os.path.exists(final_dir):
            os.mkdir(final_dir)
        shutil.move(single_pdf, os.path.join(final_dir, os.path.basename(single_pdf)))

    except Exception as e:
        # Move unprocessed PDFs to the "Unprocessed" folder
        final_dir = os.path.join(os.getcwd(), "Unprocessed")
        if not os.path.exists(final_dir):
            os.mkdir(final_dir)
        shutil.move(single_pdf, os.path.join(final_dir, os.path.basename(single_pdf)))
        print(f"Error processing {single_pdf}: {e}")

# Save the final concatenated DataFrame to CSV
final_df = pd.concat(all_data, ignore_index=True)
final_df.to_csv("Resume.csv", index=False)
